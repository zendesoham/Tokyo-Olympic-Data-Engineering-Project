

ETL Data Pipeline using Azure Cloud Services

- Objective: Developed a scalable ETL (Extract, Transform, Load) data pipeline to process and transform data using Azure services.
  
- Technologies: Azure Data Factory, Azure Data Lake Storage, Databricks (Apache Spark), Azure Synapse Analytics, Power BI.
  
- Process Overview:
  - Data Ingestion: Extracted data from various sources using Azure Data Factory, which allows seamless integration and basic transformations.
  - Storage: Loaded raw data into Azure Data Lake Storage (ADLS) for both structured and unstructured data storage.
  - Transformation: Utilized Apache Spark on Azure Databricks to perform data cleansing, transformation, and applying business logic. Cleaned data was stored back in ADLS.
  - Analytics: Used Azure Synapse Analytics for querying transformed data, performing advanced analytics using SQL and notebooks.
  - Visualization: Generated interactive dashboards using Power BI to visualize key insights from the processed data.
  
- Outcome: Efficiently processed large datasets with streamlined data transformation, enabling real-time insights and decision-making through dynamic dashboards.

- Project Architecture
  
![Tokyo_Olympic_Project_Architecture](https://github.com/user-attachments/assets/9234957f-b1b2-4051-95f6-46ac623de7b7)
